{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import Stemmer\n",
    "\n",
    "# Other\n",
    "import re\n",
    "import timeit\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      "index                 24783 non-null int64\n",
      "count                 24783 non-null int64\n",
      "hate_speech           24783 non-null int64\n",
      "offensive_language    24783 non-null int64\n",
      "neither               24783 non-null int64\n",
      "class                 24783 non-null int64\n",
      "tweet                 24783 non-null object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"labeled_data.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "filepath = \"labeled_data.csv\"\n",
    "op_file = \"cleaned_tweets.csv\"\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "stemmer = Stemmer.Stemmer('english', 100000)    \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Remove puncuation\n",
    "    #text = text.translate(string.punctuation)\n",
    "    \n",
    "    #split based on everything except a-z0-9_'.\\-\n",
    "    tokens = re.findall(\"[a-z0-9_'.\\-]+\", text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    \n",
    "    tokens = [stemmer.stemWord(w) for w in tokens if not w in stopWords and len(w) > 2 and len(w)<20]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def build_data(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    data['hate+offensive_count'] = df['offensive_language'] + df['hate_speech']\n",
    "    data['non-hate_count'] = df['neither']\n",
    "    \n",
    "    classes = []\n",
    "    for index, row in data.iterrows():\n",
    "        temp = 1 if row['hate+offensive_count'] > row['non-hate_count'] else 0\n",
    "        classes.append(temp)\n",
    "        \n",
    "    data['class'] = classes\n",
    "    \n",
    "    #label =  {1:hate, 0:non-hate}\n",
    "    labels = data['class'].map(lambda x : 1 if int(x) == 1 else 0)\n",
    "    \n",
    "    #cleaning text\n",
    "    data['tweet'] = df['tweet'].map(lambda x: clean_text(x))\n",
    "\n",
    "    data.to_csv(op_file)\n",
    "    \n",
    "    return (data, labels)\n",
    "\n",
    "data, labels = build_data(filepath)\n",
    "print(\"Done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 4 columns):\n",
      "hate+offensive_count    24783 non-null int64\n",
      "non-hate_count          24783 non-null int64\n",
      "class                   24783 non-null int64\n",
      "tweet                   24783 non-null object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 774.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    20620\n",
       "0     4163\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "data.head(10)\n",
    "data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "24783\n",
      "<class 'pandas.core.series.Series'>\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(type(data['tweet']))\n",
    "print(len(data['tweet']))\n",
    "\n",
    "print(type(data['tweet'][0:10]))\n",
    "print(len(data['tweet'][0:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sample(sample):\n",
    "    \n",
    "    print(\"\\nGiven sample size:\", len(sample))\n",
    "    \n",
    "    #Keras tokenizer function to tokenize the strings and \n",
    "    #‘texts_to_sequences’ to make sequences of words.\n",
    "\n",
    "    vocabulary_size = 20000\n",
    "\n",
    "    #Maximum number of words to work with \n",
    "    #if set, tokenization will be restricted to the top nb_words most common words in the dataset).\n",
    "    tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "\n",
    "    #fit_on_texts(texts):\n",
    "    #Arguments: list of texts to train on.\n",
    "    #tokenizer.fit_on_texts(data['tweet'])\n",
    "    tokenizer.fit_on_texts(sample)\n",
    "\n",
    "    #texts_to_sequences(texts)\n",
    "    #texts: list of texts to turn to sequences.\n",
    "    #Return: list of sequences (one per text input).\n",
    "    \n",
    "    #sequences = tokenizer.texts_to_sequences(data['tweet'])\n",
    "    sequences = tokenizer.texts_to_sequences(sample)\n",
    "    sample = pad_sequences(sequences, maxlen=50)\n",
    "    \n",
    "    print(\"Processed sample shape:\", sample.shape)\n",
    "    #print(\"Sample1:\", sample[0])\n",
    "    \n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data len: 19826 \n",
      "Test data len: 4957\n",
      "\n",
      "Given sample size: 19826\n",
      "Processed sample shape: (19826, 50)\n",
      "\n",
      "Given sample size: 4957\n",
      "Processed sample shape: (4957, 50)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sklearn.model_selection.train_test_split(*arrays, **options)[source]\n",
    "\n",
    "#*arrays : sequence of indexables with same length / shape[0]\n",
    "#Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "\n",
    "#test_size/train_size \n",
    "#If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "#If int, represents the absolute number of test samples. \n",
    "#If None, the value is set to the complement of the train size\n",
    "\n",
    "#shuffle : boolean, optional (default=True)\n",
    "#Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "\n",
    "#Returns splitting : list, length=2 * len(arrays)\n",
    "#List containing train-test split of inputs.\n",
    "\n",
    "\n",
    "X = data['tweet']\n",
    "Y = labels\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\n",
    "\n",
    "print(\"Train data len:\", len(X_train), \"\\nTest data len:\", len(X_test))\n",
    "\n",
    "X_train_seq = process_sample(X_train)\n",
    "X_test_seq = process_sample(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> \n",
      "\n",
      "yuh bitch ass ull get smack quick |Y: 1\n",
      "imnottwitfam deal cyber bull log outsid that stop pussi |Y: 1\n",
      "even mom know fuck ghetto peopl |Y: 1\n",
      "colinfreez isi support america jihadi next door http t.co aimv7gib24 mlnestel vocativ http t.co ajuau5ywiv |Y: 0\n",
      "woridstarhlphop phone goe class teacher say turn trash song mixtap https 8230 |Y: 0\n",
      "rino trash like speakerboehn karlrov frankluntz see conserv democrat parti see blacks. slave property. tcot |Y: 1\n",
      "burrgo hang hoe gonna turn hoe |Y: 1\n",
      "pablo_baaih good pussi pussi make realli think bout make breakfast pussi make think father |Y: 1\n",
      "harmssi strangekeith 8220 quanndadon word tan1aaa zero hoe 8221 |Y: 1\n",
      "wow fucken bradley suck ass. coach said niggah play barcelona hahahahahaha |Y: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23242                    yuh bitch ass ull get smack quick\n",
       "15634    imnottwitfam deal cyber bull log outsid that s...\n",
       "8964                       even mom know fuck ghetto peopl\n",
       "14751    colinfreez isi support america jihadi next doo...\n",
       "18013    woridstarhlphop phone goe class teacher say tu...\n",
       "13974    rino trash like speakerboehn karlrov franklunt...\n",
       "14544                       burrgo hang hoe gonna turn hoe\n",
       "16835    pablo_baaih good pussi pussi make realli think...\n",
       "15439    harmssi strangekeith 8220 quanndadon word tan1...\n",
       "22993    wow fucken bradley suck ass. coach said niggah...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_train), type(X_train_seq), \"\\n\")\n",
    "\n",
    "for i in range(10):\n",
    "    print(X_train.iat[i], \"|Y:\", Y_train.iat[i])\n",
    "    \n",
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yuh bitch ass ull get smack quick |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0 2616    1   14 9810   11  677  554] \n",
      "\n",
      "imnottwitfam deal cyber bull log outsid that stop pussi |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 4846\n",
      "  744 6344 1282 1721  569   31   66    9] \n",
      "\n",
      "even mom know fuck ghetto peopl |Y: 1 :--\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  56 223  22   8 106  61] \n",
      "\n",
      "colinfreez isi support america jihadi next door http t.co aimv7gib24 mlnestel vocativ http t.co ajuau5ywiv |Y: 0 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 9811  598  499  540  266  172  570    6    4\n",
      "    5 9812 9813 3942    6    4    5 9814] \n",
      "\n",
      "woridstarhlphop phone goe class teacher say turn trash song mixtap https 8230 |Y: 0 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 2169  213  447  389\n",
      "  678   29  142   19  248 1416  138   18] \n",
      "\n",
      "rino trash like speakerboehn karlrov frankluntz see conserv democrat parti see blacks. slave property. tcot |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 4847   19    7 9815 9816 9817   44\n",
      " 1983  875  373   44 9818 2170 6345  611] \n",
      "\n",
      "burrgo hang hoe gonna turn hoe |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0 9819  571    2   90  142    2] \n",
      "\n",
      "pablo_baaih good pussi pussi make realli think bout make breakfast pussi make think father |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0 6346 9820   42    9    9   28   52\n",
      "   37   72   28 1224    9   28   37 1340] \n",
      "\n",
      "harmssi strangekeith 8220 quanndadon word tan1aaa zero hoe 8221 |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 9821\n",
      " 9822   12 9823  211 6347 1501    2   13] \n",
      "\n",
      "wow fucken bradley suck ass. coach said niggah play barcelona hahahahahaha |Y: 1 :--\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0  438 1849 9824\n",
      "  173   14 1341   60   79   74 9825 6348] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sequence for sentences\n",
    "for i in range(10):\n",
    "    print(X_train.iat[i], \"|Y:\", Y_train.iat[i], \":--\\n\", X_train_seq[i], \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train data len: 19826 , Test data len: 4957\n",
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 19826 samples, validate on 4957 samples\n",
      "Epoch 1/3\n",
      "19826/19826 [==============================] - 4s 188us/step - loss: 3.2743 - acc: 0.7934 - val_loss: 2.7340 - val_acc: 0.8285\n",
      "Epoch 2/3\n",
      "19826/19826 [==============================] - 2s 88us/step - loss: 2.6773 - acc: 0.8318 - val_loss: 2.7339 - val_acc: 0.8285\n",
      "Epoch 3/3\n",
      "19826/19826 [==============================] - 2s 85us/step - loss: 2.6707 - acc: 0.8324 - val_loss: 2.7338 - val_acc: 0.8285\n",
      "\n",
      "\n",
      "Total training time: 7.8478 seconds.\n",
      "4957/4957 [==============================] - 0s 28us/step\n",
      "\n",
      "Testing time: 0.1434 seconds.\n",
      "\n",
      "Test score: 2.7338222154379137\n",
      "Test accuracy: 0.8285253178647672\n"
     ]
    }
   ],
   "source": [
    "#MLP Network architecture\n",
    "\n",
    "print('Building model...')\n",
    "model_mlp = Sequential()\n",
    "\n",
    "max_features = 20000 #size of vocab\n",
    "\n",
    "#Embedding(input_dim, output_dim, embeddings_initializer='uniform', ***, input_length=None)\n",
    "#o/p will be model.output_shape == (None, 10 :input_dim, 64:output_dim), where None is the batch dimension of the matrix given.\n",
    "#model_mlp.add(Embedding(max_features, 100, input_length=50))\n",
    "\n",
    "## Dense(64) is a fully-connected layer with 64 hidden units.\n",
    "# in the first layer, you must specify the expected input data shape: here, 20-dimensional vectors.\n",
    "model_mlp.add(Dense(64, input_dim= 50, activation='relu'))\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "model_mlp.add(Dense(64, activation='relu'))\n",
    "model_mlp.add(Dropout(0.5))\n",
    "\n",
    "model_mlp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_mlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Train data len:\", len(X_train), \", Test data len:\", len(X_test))\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "model_mlp.fit(X_train_seq, Y_train,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              validation_data=(X_test_seq, Y_test))\n",
    "\n",
    "#model_lstm.fit(data, np.array(labels), validation_split=0.2, epochs=3)\n",
    "\n",
    "print(\"\\n\\nTotal training time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "\n",
    "start = timeit.default_timer()\n",
    "score, acc = model_mlp.evaluate(X_test_seq, Y_test, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(\"\\nTesting time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "print('\\nTest score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, 50, 1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlp.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Train data len: 19826 , Test data len: 4957\n",
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 19826 samples, validate on 4957 samples\n",
      "Epoch 1/3\n",
      "19826/19826 [==============================] - 78s 4ms/step - loss: 0.1769 - acc: 0.9279 - val_loss: 0.3585 - val_acc: 0.8572\n",
      "Epoch 2/3\n",
      "19826/19826 [==============================] - 87s 4ms/step - loss: 0.1051 - acc: 0.9602 - val_loss: 0.3682 - val_acc: 0.8439\n",
      "Epoch 3/3\n",
      "19826/19826 [==============================] - 87s 4ms/step - loss: 0.0953 - acc: 0.9640 - val_loss: 0.4175 - val_acc: 0.8449\n",
      "\n",
      "\n",
      "Total training time: 254.2815 seconds.\n",
      "4957/4957 [==============================] - 5s 951us/step\n",
      "\n",
      "Testing time: 4.7194 seconds.\n",
      "\n",
      "Test score: 0.41748238686642775\n",
      "Test accuracy: 0.8448658461938203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#LSTM Network architecture\n",
    "print('Building model...')\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#The network starts with an embedding layer.\n",
    "#Turns positive integers (indexes) into dense vectors of fixed size allowing the n/w to represent a word in a meaningful way.\n",
    "#eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "#This layer can only be used as the first layer in a model.\n",
    "\n",
    "#keras.layers.Embedding(input_dim, output_dim, embeddings_initializer='uniform', ***, input_length=None)\n",
    "\n",
    "#input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "\n",
    "#output_dim: int >= 0. Dimension of the dense embedding.\n",
    "\n",
    "#input_length: Length of input sequences, when it is constant. \n",
    "#This argument is required if you are going to connect Flatten then Dense layers upstream \n",
    "#(without it, the shape of the dense outputs cannot be computed).\n",
    "\n",
    "#eg. model.add(Embedding(1000, 64, input_length=10))\n",
    "\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# where the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n",
    "\n",
    "# o/p will be model.output_shape == (None, 10 :input_dim, 64:output_dim), where None is the batch dimension of the matrix given.\n",
    "\n",
    "\n",
    "model_lstm.add(Embedding(20000, 100, input_length=50))\n",
    "model_lstm.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "print(\"Train data len:\", len(X_train), \", Test data len:\", len(X_test))\n",
    "\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "model_lstm.fit(X_train_seq, Y_train,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              validation_data=(X_test_seq, Y_test))\n",
    "\n",
    "#model_lstm.fit(data, np.array(labels), validation_split=0.2, epochs=3)\n",
    "\n",
    "print(\"\\n\\nTotal training time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "score, acc = model_lstm.evaluate(X_test_seq, Y_test, batch_size = BATCH_SIZE)\n",
    "\n",
    "print(\"\\nTesting time: %.4f seconds.\" % (timeit.default_timer() - start))\n",
    "print('\\nTest score:', score)\n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'> <class 'numpy.ndarray'> <class 'pandas.core.series.Series'>\n",
      "19826 4957\n",
      "(24783,)\n",
      "(24783, 50)\n",
      "(24783,)\n",
      "(24783, 51)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2616</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>9810</td>\n",
       "      <td>11</td>\n",
       "      <td>677</td>\n",
       "      <td>554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4846</td>\n",
       "      <td>744</td>\n",
       "      <td>6344</td>\n",
       "      <td>1282</td>\n",
       "      <td>1721</td>\n",
       "      <td>569</td>\n",
       "      <td>31</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>223</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>106</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9812</td>\n",
       "      <td>9813</td>\n",
       "      <td>3942</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>9814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>389</td>\n",
       "      <td>678</td>\n",
       "      <td>29</td>\n",
       "      <td>142</td>\n",
       "      <td>19</td>\n",
       "      <td>248</td>\n",
       "      <td>1416</td>\n",
       "      <td>138</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>44</td>\n",
       "      <td>1983</td>\n",
       "      <td>875</td>\n",
       "      <td>373</td>\n",
       "      <td>44</td>\n",
       "      <td>9818</td>\n",
       "      <td>2170</td>\n",
       "      <td>6345</td>\n",
       "      <td>611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9819</td>\n",
       "      <td>571</td>\n",
       "      <td>2</td>\n",
       "      <td>90</td>\n",
       "      <td>142</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>52</td>\n",
       "      <td>37</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "      <td>1224</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>37</td>\n",
       "      <td>1340</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9821</td>\n",
       "      <td>9822</td>\n",
       "      <td>12</td>\n",
       "      <td>9823</td>\n",
       "      <td>211</td>\n",
       "      <td>6347</td>\n",
       "      <td>1501</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9824</td>\n",
       "      <td>173</td>\n",
       "      <td>14</td>\n",
       "      <td>1341</td>\n",
       "      <td>60</td>\n",
       "      <td>79</td>\n",
       "      <td>74</td>\n",
       "      <td>9825</td>\n",
       "      <td>6348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9  ...      41    42    43    44    45    46  \\\n",
       "0  0  0  0  0  0  0  0  0  0  0  ...       0     0  2616     1    14  9810   \n",
       "1  0  0  0  0  0  0  0  0  0  0  ...    4846   744  6344  1282  1721   569   \n",
       "2  0  0  0  0  0  0  0  0  0  0  ...       0     0     0    56   223    22   \n",
       "3  0  0  0  0  0  0  0  0  0  0  ...       4     5  9812  9813  3942     6   \n",
       "4  0  0  0  0  0  0  0  0  0  0  ...     389   678    29   142    19   248   \n",
       "5  0  0  0  0  0  0  0  0  0  0  ...      44  1983   875   373    44  9818   \n",
       "6  0  0  0  0  0  0  0  0  0  0  ...       0     0     0  9819   571     2   \n",
       "7  0  0  0  0  0  0  0  0  0  0  ...      52    37    72    28  1224     9   \n",
       "8  0  0  0  0  0  0  0  0  0  0  ...    9821  9822    12  9823   211  6347   \n",
       "9  0  0  0  0  0  0  0  0  0  0  ...    9824   173    14  1341    60    79   \n",
       "\n",
       "     47    48    49  class  \n",
       "0    11   677   554      1  \n",
       "1    31    66     9      1  \n",
       "2     8   106    61      1  \n",
       "3     4     5  9814      0  \n",
       "4  1416   138    18      0  \n",
       "5  2170  6345   611      1  \n",
       "6    90   142     2      1  \n",
       "7    28    37  1340      1  \n",
       "8  1501     2    13      1  \n",
       "9    74  9825  6348      1  \n",
       "\n",
       "[10 rows x 51 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(X_train), type(X_train_seq), type(labels))\n",
    "\n",
    "print(len(X_train_seq), len(X_test_seq))\n",
    "print(labels.shape)\n",
    "\n",
    "data_x = pd.concat([pd.DataFrame(X_train_seq), pd.DataFrame(X_test_seq)] , axis = 0)\n",
    "print(data_x.shape)\n",
    "\n",
    "data_y = pd.concat([Y_train, Y_test] , axis = 0)\n",
    "print(data_y.shape)\n",
    "\n",
    "#to handle InvalidIndexError: Reindexing only valid with uniquely valued Index objects\n",
    "data_x.reset_index(inplace=True, drop=True)\n",
    "data_y.reset_index(inplace=True, drop=True)\n",
    "\n",
    "data_save = pd.concat([data_x, data_y], axis = 1)\n",
    "print(data_save.shape)\n",
    "\n",
    "data_save.to_csv('vector_labels.csv', index=False)\n",
    "\n",
    "data_save.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "ToC",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
