{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LeakyReLU, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import Stemmer\n",
    "\n",
    "# Other\n",
    "import os,re\n",
    "import timeit\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12315 entries, 0 to 12314\n",
      "Data columns (total 4 columns):\n",
      "id       12315 non-null int64\n",
      "label    12315 non-null object\n",
      "name     12315 non-null object\n",
      "text     12315 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 384.9+ KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572342978255048705</td>\n",
       "      <td>racism</td>\n",
       "      <td>thefoxbandit</td>\n",
       "      <td>so drasko just said he was impressed the girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572341498827522049</td>\n",
       "      <td>racism</td>\n",
       "      <td>patricia</td>\n",
       "      <td>drasko they didn't cook half a bird you idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572340476503724032</td>\n",
       "      <td>racism</td>\n",
       "      <td>food</td>\n",
       "      <td>hopefully someone cooks drasko in the next ep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572334712804384768</td>\n",
       "      <td>racism</td>\n",
       "      <td>lil</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>572332655397629952</td>\n",
       "      <td>racism</td>\n",
       "      <td>jlwhitaker</td>\n",
       "      <td>these girls are the equivalent of the irritati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>575949086055997440</td>\n",
       "      <td>racism</td>\n",
       "      <td>dominiquew</td>\n",
       "      <td>#mkr  lost the plot - where's the big texan wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>551659627872415744</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@lauracdean i love how the islamofascists recr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>551763146877452288</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@ibnhlophe @eeviewonders @anjemchoudary a crim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>551768543277355009</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary nothing desperate about it. hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>551769061055811584</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary idiots like you making such dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>551769698917167104</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary your prophet was a rapist, murd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>551779988567891969</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary maybe you can give a talk about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>551783732466577408</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@jonsnowc4 @swatigauri when an islamolunatic b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>551787027725705216</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 israel is only trying to defend itsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>551787273256050688</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 the muslims have exterminated christi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>551787717168619521</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 muslims are fighting with every relig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>551796768623435776</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>it seems that allah sits around all day obsess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>551797843472556032</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>in islam women must be locked in their houses,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>551798472827883520</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>islam considers women as one big sex organ, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>551799120080293888</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>to muslims a woman walking down the street is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   label          name  \\\n",
       "0   572342978255048705  racism  thefoxbandit   \n",
       "1   572341498827522049  racism      patricia   \n",
       "2   572340476503724032  racism          food   \n",
       "3   572334712804384768  racism           lil   \n",
       "4   572332655397629952  racism    jlwhitaker   \n",
       "5   575949086055997440  racism    dominiquew   \n",
       "6   551659627872415744  racism    vile_islam   \n",
       "7   551763146877452288  racism    vile_islam   \n",
       "8   551768543277355009  racism    vile_islam   \n",
       "9   551769061055811584  racism    vile_islam   \n",
       "10  551769698917167104  racism    vile_islam   \n",
       "11  551779988567891969  racism    vile_islam   \n",
       "12  551783732466577408  racism    vile_islam   \n",
       "13  551787027725705216  racism    vile_islam   \n",
       "14  551787273256050688  racism    vile_islam   \n",
       "15  551787717168619521  racism    vile_islam   \n",
       "16  551796768623435776  racism    vile_islam   \n",
       "17  551797843472556032  racism    vile_islam   \n",
       "18  551798472827883520  racism    vile_islam   \n",
       "19  551799120080293888  racism    vile_islam   \n",
       "\n",
       "                                                 text  \n",
       "0   so drasko just said he was impressed the girls...  \n",
       "1   drasko they didn't cook half a bird you idiot ...  \n",
       "2   hopefully someone cooks drasko in the next ep ...  \n",
       "3   of course you were born in serbia...you're as ...  \n",
       "4   these girls are the equivalent of the irritati...  \n",
       "5   #mkr  lost the plot - where's the big texan wi...  \n",
       "6   @lauracdean i love how the islamofascists recr...  \n",
       "7   @ibnhlophe @eeviewonders @anjemchoudary a crim...  \n",
       "8   @anjemchoudary nothing desperate about it. hum...  \n",
       "9   @anjemchoudary idiots like you making such dec...  \n",
       "10  @anjemchoudary your prophet was a rapist, murd...  \n",
       "11  @anjemchoudary maybe you can give a talk about...  \n",
       "12  @jonsnowc4 @swatigauri when an islamolunatic b...  \n",
       "13  @remy119 israel is only trying to defend itsel...  \n",
       "14  @remy119 the muslims have exterminated christi...  \n",
       "15  @remy119 muslims are fighting with every relig...  \n",
       "16  it seems that allah sits around all day obsess...  \n",
       "17  in islam women must be locked in their houses,...  \n",
       "18  islam considers women as one big sex organ, so...  \n",
       "19  to muslims a woman walking down the street is ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_path = \"encoded_data/\"\n",
    "\n",
    "tweet_df = pd.read_pickle(encoded_data_path + \"tweets.pkl\")\n",
    "\n",
    "tweet_df.info()\n",
    "print(\"\\n\")\n",
    "tweet_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id\n",
      "count  1.231500e+04\n",
      "mean   5.619213e+17\n",
      "std    4.051089e+16\n",
      "min    3.193999e+17\n",
      "25%    5.641296e+17\n",
      "50%    5.710391e+17\n",
      "75%    5.756380e+17\n",
      "max    6.847790e+17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neither    5850\n",
       "sexism     4341\n",
       "racism     2074\n",
       "both         50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df.describe())\n",
    "\n",
    "tweet_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603528285247635456</td>\n",
       "      <td>neither</td>\n",
       "      <td>trash</td>\n",
       "      <td>@themikewebster cool i feel petty but also he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564049824274989058</td>\n",
       "      <td>sexism</td>\n",
       "      <td>male</td>\n",
       "      <td>@bmackillopthio but, trump gets called a megal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563905276496334849</td>\n",
       "      <td>sexism</td>\n",
       "      <td>male</td>\n",
       "      <td>.@blackopal80 then what really opened my eyes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>563797959729352704</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>leo was freaking out because of the wind, so i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>567765262713298944</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@keegannyc and how about if we investigate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>568231567698472961</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@frozenwes i hate numpads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>572108345504501760</td>\n",
       "      <td>sexism</td>\n",
       "      <td>gynecologic</td>\n",
       "      <td>rt @federaltyranny: sexist femi nazi aborts ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>573770498837872640</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@mystyr_e we'll have more info out about this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>574773417213173760</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@witnesstoallah the poor women do it because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495020834495676417</td>\n",
       "      <td>sexism</td>\n",
       "      <td>@yesyouresexist</td>\n",
       "      <td>rt @nearthetown: @viragobooks @guardian   work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>572343058529828864</td>\n",
       "      <td>neither</td>\n",
       "      <td>✌</td>\n",
       "      <td>that #begacheese ad is so old i'm wondering if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>392322287203921920</td>\n",
       "      <td>sexism</td>\n",
       "      <td>@yesyouresexist</td>\n",
       "      <td>rt @jcsliva1999 @pjmcentee she is blond what d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>569224391914967040</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>i'm looking for a few people to help with @gga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>603686773164609536</td>\n",
       "      <td>sexism</td>\n",
       "      <td>video</td>\n",
       "      <td>#blameonenotall, because if all men can be jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>571006220451991552</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@peerworker @evvykube you are changing your mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>575548222204178433</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@pnibbler @alternet @maxblumenthal mohammed ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>556962076153757696</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@politics_pr @foxnews islamic no go zones are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>602181135435706369</td>\n",
       "      <td>neither</td>\n",
       "      <td>golfertx</td>\n",
       "      <td>hate to think that's exactly where the meet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>571404378692837377</td>\n",
       "      <td>sexism</td>\n",
       "      <td>m</td>\n",
       "      <td>@frootsicle he interrupted the girl who starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>596165409029496833</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@warrior_tank randi#1266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id    label             name  \\\n",
       "0   603528285247635456  neither            trash   \n",
       "1   564049824274989058   sexism             male   \n",
       "2   563905276496334849   sexism             male   \n",
       "3   563797959729352704  neither       misandrist   \n",
       "4   567765262713298944   racism       vile_islam   \n",
       "5   568231567698472961  neither       misandrist   \n",
       "6   572108345504501760   sexism      gynecologic   \n",
       "7   573770498837872640  neither       misandrist   \n",
       "8   574773417213173760   racism       vile_islam   \n",
       "9   495020834495676417   sexism  @yesyouresexist   \n",
       "10  572343058529828864  neither                ✌   \n",
       "11  392322287203921920   sexism  @yesyouresexist   \n",
       "12  569224391914967040  neither       misandrist   \n",
       "13  603686773164609536   sexism            video   \n",
       "14  571006220451991552  neither       misandrist   \n",
       "15  575548222204178433   racism       vile_islam   \n",
       "16  556962076153757696   racism       vile_islam   \n",
       "17  602181135435706369  neither         golfertx   \n",
       "18  571404378692837377   sexism                m   \n",
       "19  596165409029496833  neither       misandrist   \n",
       "\n",
       "                                                 text  \n",
       "0   @themikewebster cool i feel petty but also he ...  \n",
       "1   @bmackillopthio but, trump gets called a megal...  \n",
       "2   .@blackopal80 then what really opened my eyes....  \n",
       "3   leo was freaking out because of the wind, so i...  \n",
       "4   @keegannyc and how about if we investigate the...  \n",
       "5                          @frozenwes i hate numpads.  \n",
       "6   rt @federaltyranny: sexist femi nazi aborts ma...  \n",
       "7   @mystyr_e we'll have more info out about this ...  \n",
       "8   @witnesstoallah the poor women do it because t...  \n",
       "9   rt @nearthetown: @viragobooks @guardian   work...  \n",
       "10  that #begacheese ad is so old i'm wondering if...  \n",
       "11  rt @jcsliva1999 @pjmcentee she is blond what d...  \n",
       "12  i'm looking for a few people to help with @gga...  \n",
       "13  #blameonenotall, because if all men can be jud...  \n",
       "14  @peerworker @evvykube you are changing your mi...  \n",
       "15  @pnibbler @alternet @maxblumenthal mohammed ra...  \n",
       "16  @politics_pr @foxnews islamic no go zones are ...  \n",
       "17  hate to think that's exactly where the meet co...  \n",
       "18  @frootsicle he interrupted the girl who starte...  \n",
       "19                           @warrior_tank randi#1266  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling \n",
    "tweet_df = tweet_df.sample(frac=1).reset_index(drop=True)\n",
    "tweet_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id\n",
      "count  1.231500e+04\n",
      "mean   5.619213e+17\n",
      "std    4.051089e+16\n",
      "min    3.193999e+17\n",
      "25%    5.641296e+17\n",
      "50%    5.710391e+17\n",
      "75%    5.756380e+17\n",
      "max    6.847790e+17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neither    5850\n",
       "sexism     4341\n",
       "racism     2074\n",
       "both         50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df.describe())\n",
    "tweet_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "op_file = \"dataset/cleaned_tweet_data.csv\"\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "stemmer = Stemmer.Stemmer('english', 100000)    \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Remove puncuation\n",
    "    #text = text.translate(string.punctuation)\n",
    "    \n",
    "    #split based on everything except a-z0-9_'.\\-\n",
    "    tokens = re.findall(\"[a-z0-9_'.\\-]+\", text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    \n",
    "    tokens = [stemmer.stemWord(w) for w in tokens if not w in stopWords and len(w) > 2 and len(w)<20]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def build_data(df):\n",
    "      \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    #cleaning text\n",
    "    data['tweet'] = df['text'].map(lambda x: clean_text(x))    \n",
    "    \n",
    "    data['label'] = df['label']\n",
    "    \n",
    "    data['racism'] = df['label'].map(lambda x: 1 if x == 'racism' else 0)\n",
    "    data['sexism'] = df['label'].map(lambda x: 1 if x == 'sexism' else 0)\n",
    "    data['neither'] = df['label'].map(lambda x: 1 if x == 'neither' else 0)\n",
    "    data['both'] = df['label'].map(lambda x: 1 if x == 'both' else 0)\n",
    "    \n",
    "    # set hate 1 if label is racism/sexism/both\n",
    "    data['hate'] = df['label'].map(lambda x: 0 if x == 'neither' else 1)    \n",
    "    \n",
    "    #label =  {1:hate, 0:non-hate}\n",
    "    labels_hate = data['hate'].map(lambda x : 1 if int(x) == 1 else 0)\n",
    "    \n",
    "    #label =  {1:hate, 0:non-hate}\n",
    "    labels_racist = data['racism'].map(lambda x : 1 if int(x) == 1 else 0)\n",
    "    \n",
    "    data.to_csv(op_file)\n",
    "    \n",
    "    return (data, labels_hate, labels_racist)\n",
    "\n",
    "data, labels_hate, labels_racist  = build_data(tweet_df)\n",
    "print(\"Done!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12315 entries, 0 to 12314\n",
      "Data columns (total 7 columns):\n",
      "tweet      12315 non-null object\n",
      "label      12315 non-null object\n",
      "racism     12315 non-null int64\n",
      "sexism     12315 non-null int64\n",
      "neither    12315 non-null int64\n",
      "both       12315 non-null int64\n",
      "hate       12315 non-null int64\n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 673.6+ KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>racism</th>\n",
       "      <th>sexism</th>\n",
       "      <th>neither</th>\n",
       "      <th>both</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>themikewebst cool feel petti also rli take sid...</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmackillopthio trump get call megalomaniac. ge...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackopal80 realli open eyes. articl financi a...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leo freak wind buri luggag could sleep. http t...</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keegannyc investig hate speech quran. http t.c...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frozenw hate numpads.</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>federaltyranni sexist femi nazi abort male bab...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mystyr_ we'll info soon.</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>witnesstoallah poor women marri muslim man pre...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nearthetown viragobook guardian work virtual f...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet    label  racism  sexism  \\\n",
       "0  themikewebst cool feel petti also rli take sid...  neither       0       0   \n",
       "1  bmackillopthio trump get call megalomaniac. ge...   sexism       0       1   \n",
       "2  blackopal80 realli open eyes. articl financi a...   sexism       0       1   \n",
       "3  leo freak wind buri luggag could sleep. http t...  neither       0       0   \n",
       "4  keegannyc investig hate speech quran. http t.c...   racism       1       0   \n",
       "5                              frozenw hate numpads.  neither       0       0   \n",
       "6  federaltyranni sexist femi nazi abort male bab...   sexism       0       1   \n",
       "7                           mystyr_ we'll info soon.  neither       0       0   \n",
       "8  witnesstoallah poor women marri muslim man pre...   racism       1       0   \n",
       "9  nearthetown viragobook guardian work virtual f...   sexism       0       1   \n",
       "\n",
       "   neither  both  hate  \n",
       "0        1     0     0  \n",
       "1        0     0     1  \n",
       "2        0     0     1  \n",
       "3        1     0     0  \n",
       "4        0     0     1  \n",
       "5        1     0     0  \n",
       "6        0     0     1  \n",
       "7        1     0     0  \n",
       "8        0     0     1  \n",
       "9        0     0     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "print(\"\\n\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find correlation between columns\n",
    "\n",
    "def plot_correlation(data, size=8):\n",
    "    corr= data.corr()\n",
    "    fig, ax =plt.subplots(figsize=(size,size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neither    5850\n",
      "sexism     4341\n",
      "racism     2074\n",
      "both         50\n",
      "Name: label, dtype: int64\n",
      "\n",
      "hate(1) and non-hate(0) label count:-\n",
      "\n",
      "1    6465\n",
      "0    5850\n",
      "Name: hate, dtype: int64\n",
      "\n",
      "racism(1) and non-racism(0) label count:-\n",
      "\n",
      "0    10241\n",
      "1     2074\n",
      "Name: racism, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "print(\"\\nhate(1) and non-hate(0) label count:-\\n\")\n",
    "print(data['hate'].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nracism(1) and non-racism(0) label count:-\\n\")\n",
    "print(data['racism'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_of_tweet: 20 \n",
      "\n",
      "ive met racist people. ive met sexist people. ive never met someon treat everi man like he rapist.. blameonenotal stupid.\n",
      "\n",
      "length of top 200 tweets:-\n",
      " [20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "\n",
      "number of words:  116283\n",
      "number of unique words:  21351\n"
     ]
    }
   ],
   "source": [
    "# max tweet length\n",
    "\n",
    "tweets = data['tweet'].values\n",
    "\n",
    "max_len = 0\n",
    "max_len_tweet = \"\"\n",
    "len_data = []\n",
    "words = []\n",
    "\n",
    "for t in tweets:\n",
    "    len_data.append(len(t.split()))\n",
    "    words += t.split()\n",
    "    if len(t.split()) > max_len:\n",
    "        max_len = len(t.split())\n",
    "        max_len_tweet = t\n",
    "        \n",
    "\n",
    "VOCAB_SIZE = len(set(words))\n",
    "        \n",
    "print(\"max_len_of_tweet:\", max_len,\"\\n\")\n",
    "print(max_len_tweet)\n",
    "\n",
    "print(\"\\nlength of top 200 tweets:-\\n\",sorted(len_data, reverse = True)[:400])\n",
    "\n",
    "print(\"\\nnumber of words: \", len(words))\n",
    "print(\"number of unique words: \", VOCAB_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sample shape: (12315, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12315, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SENT_LEN2 = 20\n",
    "\n",
    "def process_data2(sample):  \n",
    "    # Keras tokenizer function to tokenize the strings and \n",
    "    #‘texts_to_sequences’ to make sequences of words.\n",
    "\n",
    "    #Maximum number of words to work with \n",
    "    #if set, tokenization will be restricted to the top nb_words most common words in the dataset).\n",
    "    tokenizer = Tokenizer(num_words= VOCAB_SIZE)\n",
    "\n",
    "    #fit_on_texts(texts):\n",
    "    #Arguments: list of texts to train on.\n",
    "    #tokenizer.fit_on_texts(data['tweet'])\n",
    "    tokenizer.fit_on_texts(sample)\n",
    "\n",
    "    #texts_to_sequences(texts)\n",
    "    #texts: list of texts to turn to sequences.\n",
    "    #Return: list of sequences (one per text input).\n",
    "    \n",
    "    #sequences = tokenizer.texts_to_sequences(data['tweet'])\n",
    "    sequences = tokenizer.texts_to_sequences(sample)\n",
    "    sample = pad_sequences(sequences, maxlen = MAX_SENT_LEN2)\n",
    "    \n",
    "    print(\"Processed sample shape:\", sample.shape)\n",
    "    #print(\"Sample1:\", sample[0])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "data_embedding2 = process_data2(data['tweet'])\n",
    "data_embedding2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themikewebst cool feel petti also rli take side someon call feminazi time so....... \n",
      "\n",
      "[   0    0    0    0    0    0    0 7909  501   88 4160  116 3429   56\n",
      "  986   65   15    7   25  439]\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][0], \"\\n\")\n",
    "print(data_embedding2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(len(data_embedding2)*.8)\n",
    "\n",
    "x2 = data_embedding2\n",
    "\n",
    "# converting to numpy array\n",
    "#y = labels.values\n",
    "y2_hate = labels_hate\n",
    "y2_racism = labels_racist\n",
    "\n",
    "#x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = split_size)\n",
    "\n",
    "x2_train = x2[:split_size]\n",
    "x2_test = x2[split_size:]\n",
    "\n",
    "y2_hate_train = y2_hate[:split_size]\n",
    "y2_hate_test = y2_hate[split_size:]\n",
    "\n",
    "y2_racism_train = y2_racism[:split_size]\n",
    "y2_racism_test = y2_racism[split_size:]\n",
    "\n",
    "# transform labels into one hot representation\n",
    "y2_hate_train_one_hot = (np.arange(np.max(y2_hate_train) + 1) == y2_hate_train[:, None]).astype(float)\n",
    "y2_hate_test_one_hot = (np.arange(np.max(y2_hate_test) + 1) == y2_hate_test[:, None]).astype(float)\n",
    "\n",
    "y2_racism_train_one_hot = (np.arange(np.max(y2_racism_train) + 1) == y2_racism_train[:, None]).astype(float)\n",
    "y2_racism_test_one_hot = (np.arange(np.max(y2_racism_test) + 1) == y2_racism_test[:, None]).astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate Train split:\n",
      "1    5172\n",
      "0    4680\n",
      "Name: hate, dtype: int64\n",
      "1    0.52497\n",
      "0    0.47503\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "\n",
      "Hate Test split:\n",
      "1    1293\n",
      "0    1170\n",
      "Name: hate, dtype: int64\n",
      "1    0.52497\n",
      "0    0.47503\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "Racism Train split:\n",
      "0    8202\n",
      "1    1650\n",
      "Name: racism, dtype: int64\n",
      "0    0.832521\n",
      "1    0.167479\n",
      "Name: racism, dtype: float64\n",
      "\n",
      "\n",
      "Racism Test split:\n",
      "0    2039\n",
      "1     424\n",
      "Name: racism, dtype: int64\n",
      "0    0.827852\n",
      "1    0.172148\n",
      "Name: racism, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Hate Train split:\")\n",
    "print(y2_hate_train.value_counts())\n",
    "print(y2_hate_train.value_counts()/len(y2_hate_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Hate Test split:\")\n",
    "print(y2_hate_test.value_counts())\n",
    "print(y2_hate_test.value_counts()/len(y2_hate_test))\n",
    "\n",
    "\n",
    "print(\"\\nRacism Train split:\")\n",
    "print(y2_racism_train.value_counts())\n",
    "print(y2_racism_train.value_counts()/len(y2_racism_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Racism Test split:\")\n",
    "print(y2_racism_test.value_counts())\n",
    "print(y2_racism_test.value_counts()/len(y2_racism_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 9852 samples, validate on 2463 samples\n",
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 58s 6ms/step - loss: 0.3750 - acc: 0.8235 - val_loss: 0.2676 - val_acc: 0.8961\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.89606, saving model to encoded_data/weights-improvement-01-0.8961.hdf5\n",
      "Epoch 2/10\n",
      "9852/9852 [==============================] - 55s 6ms/step - loss: 0.1355 - acc: 0.9535 - val_loss: 0.2571 - val_acc: 0.9054\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.89606 to 0.90540, saving model to encoded_data/weights-improvement-02-0.9054.hdf5\n",
      "Epoch 3/10\n",
      "9852/9852 [==============================] - 53s 5ms/step - loss: 0.0604 - acc: 0.9796 - val_loss: 0.2920 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.90540\n",
      "Epoch 4/10\n",
      "9852/9852 [==============================] - 53s 5ms/step - loss: 0.0368 - acc: 0.9884 - val_loss: 0.3224 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.90540\n",
      "Epoch 5/10\n",
      "9852/9852 [==============================] - 53s 5ms/step - loss: 0.0209 - acc: 0.9931 - val_loss: 0.3504 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.90540\n",
      "Epoch 6/10\n",
      "5280/9852 [===============>..............] - ETA: 23s - loss: 0.0197 - acc: 0.9941"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-621433b814d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               validation_data = (x2_test, y2_hate_test_one_hot))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 21351\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM, input_length = INP_DIM))\n",
    "model.add(LSTM(200, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "\n",
    "model.fit(x2_train, y2_hate_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_hate_test_one_hot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463,) (2463, 2) <class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1024  142]\n",
      " [  91 1206]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.88      0.90      1166\n",
      "          1       0.89      0.93      0.91      1297\n",
      "\n",
      "avg / total       0.91      0.91      0.91      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm_hate = load_model(encoded_data_path +  'weights-improvement-02-0.9054.hdf5')\n",
    "\n",
    "y2_predicted = model_lstm_hate.predict(x2_test)\n",
    "print(y2_hate_test.shape, y2_predicted.shape, type(y2_hate_test), type(y2_predicted))\n",
    "\n",
    "y2_predicted_ = y2_predicted.argmax(axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_hate_test, y2_predicted_))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_hate_test, y2_predicted_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 9852 samples, validate on 2463 samples\n",
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 162s 16ms/step - loss: 0.1311 - acc: 0.9578 - val_loss: 0.0770 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97726, saving model to encoded_data/weights-racism-improvement-01-0.9773.hdf5\n",
      "Epoch 2/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0206 - acc: 0.9938 - val_loss: 0.0904 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97726 to 0.97808, saving model to encoded_data/weights-racism-improvement-02-0.9781.hdf5\n",
      "Epoch 3/10\n",
      "9852/9852 [==============================] - 149s 15ms/step - loss: 0.0112 - acc: 0.9975 - val_loss: 0.0988 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.97808\n",
      "Epoch 4/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1174 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97808\n",
      "Epoch 5/10\n",
      "9852/9852 [==============================] - 148s 15ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.1319 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97808 to 0.97929, saving model to encoded_data/weights-racism-improvement-05-0.9793.hdf5\n",
      "Epoch 6/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.1524 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97929\n",
      "Epoch 7/10\n",
      " 152/9852 [..............................] - ETA: 2:29 - loss: 1.1686e-04 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1878189674cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               validation_data = (x2_test, y2_racism_test_one_hot))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 21351\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM, input_length = INP_DIM))\n",
    "model.add(LSTM(200, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"weights-racism-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "\n",
    "model.fit(x2_train, y2_racism_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_racism_test_one_hot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463,) (2463, 2) <class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2037   16]\n",
      " [  35  375]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      2053\n",
      "          1       0.96      0.91      0.94       410\n",
      "\n",
      "avg / total       0.98      0.98      0.98      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm_racism = load_model(encoded_data_path +  'weights-racism-improvement-05-0.9793.hdf5')\n",
    "\n",
    "y2_predicted = model_lstm_racism.predict(x2_test)\n",
    "print(y2_racism_test.shape, y2_predicted.shape, type(y2_racism_test), type(y2_predicted))\n",
    "\n",
    "y2_predicted_ = y2_predicted.argmax(axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_racism_test, y2_predicted_))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_racism_test, y2_predicted_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 126s 13ms/step - loss: -12.5113 - hate_output_loss: 0.4888 - race_output_loss: 13.0000\n",
      "Epoch 2/10\n",
      "9852/9852 [==============================] - 116s 12ms/step - loss: -13.1639 - hate_output_loss: 0.1539 - race_output_loss: 13.3178\n",
      "Epoch 3/10\n",
      "9852/9852 [==============================] - 118s 12ms/step - loss: -13.2608 - hate_output_loss: 0.0605 - race_output_loss: 13.3213\n",
      "Epoch 4/10\n",
      "9852/9852 [==============================] - 117s 12ms/step - loss: -13.2932 - hate_output_loss: 0.0295 - race_output_loss: 13.3227\n",
      "Epoch 5/10\n",
      "9852/9852 [==============================] - 120s 12ms/step - loss: -13.3025 - hate_output_loss: 0.0202 - race_output_loss: 13.3227\n",
      "Epoch 6/10\n",
      "9852/9852 [==============================] - 120s 12ms/step - loss: -13.3083 - hate_output_loss: 0.0144 - race_output_loss: 13.3227\n",
      "Epoch 7/10\n",
      "9852/9852 [==============================] - 119s 12ms/step - loss: -13.3090 - hate_output_loss: 0.0137 - race_output_loss: 13.3227 58s - loss: -13.2513 - - ETA: 48s - loss: -13.3209 - \n",
      "Epoch 8/10\n",
      "9852/9852 [==============================] - 117s 12ms/step - loss: -13.3089 - hate_output_loss: 0.0138 - race_output_loss: 13.3227 57s - loss: -13.3108 - hate_output_lo - ETA: 50s - lo\n",
      "Epoch 9/10\n",
      "9852/9852 [==============================] - ETA: 0s - loss: -13.3157 - hate_output_loss: 0.0102 - race_output_loss: 13.32 - 117s 12ms/step - loss: -13.3125 - hate_output_loss: 0.0102 - race_output_loss: 13.3227\n",
      "Epoch 10/10\n",
      "9852/9852 [==============================] - 117s 12ms/step - loss: -13.3124 - hate_output_loss: 0.0104 - race_output_loss: 13.3227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x621b557a20>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INP_DIM = x2_train.shape[1]\n",
    "\n",
    "input_hate = Input(shape=(INP_DIM,), name='input_hate')\n",
    "input_race = Input(shape=(INP_DIM,), name='input_race')\n",
    "\n",
    "x_hate = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_hate)\n",
    "x_race = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_race)\n",
    "\n",
    "lstm_hate_out = LSTM(100)(x_hate)\n",
    "lstm_race_out = LSTM(100)(x_race)\n",
    "\n",
    "x = concatenate([lstm_hate_out, lstm_race_out])\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "\n",
    "hate_output = Dense(2, activation='softmax', name='hate_output')(x)\n",
    "race_output = Dense(2, activation='softmax', name='race_output')(x)\n",
    "\n",
    "#This defines a model with two inputs and two outputs:\n",
    "model = Model(inputs=[input_hate, input_race], outputs=[hate_output, race_output])\n",
    "\n",
    "\n",
    "#We compile the model and assign a weight of 0.2 to the auxiliary loss. \n",
    "#To specify different loss_weights or lossfor each different output, you can use a list or a dictionary. \n",
    "#Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.\n",
    "\"\"\"\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "\n",
    "#We can train the model by passing it lists of input arrays and target arrays:\n",
    "model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "\"\"\"\n",
    "\n",
    "#Since our inputs and outputs are named (we passed them a \"name\" argument), \n",
    "#we could also have compiled the model via\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'hate_output': 'binary_crossentropy', 'race_output': 'binary_crossentropy'},\n",
    "              loss_weights={'hate_output': 1., 'race_output': -1.0})\n",
    "\n",
    "# And trained it via:\n",
    "model.fit({'input_hate': x2_train, 'input_race': x2_train},\n",
    "          {'hate_output': y2_hate_train_one_hot, 'race_output': y2_racism_train_one_hot},\n",
    "          epochs=10, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1083   83]\n",
      " [ 179 1118]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.93      0.89      1166\n",
      "          1       0.93      0.86      0.90      1297\n",
      "\n",
      "avg / total       0.90      0.89      0.89      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix:\n",
    "[[1024  142]\n",
    " [  91 1206]]\n",
    "\n",
    "Classification Matrix:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.92      0.88      0.90      1166\n",
    "          1       0.89      0.93      0.91      1297\n",
    "\n",
    "avg / total       0.91      0.91      0.91      2463\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prediction = model.predict([x2_test, x2_test])\n",
    "x2_test.shape, prediction[0].shape, prediction[1].shape\n",
    "\n",
    "prediction = np.argmax(prediction[0], axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_hate_test, prediction))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_hate_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[   0 2053]\n",
      " [   0  410]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      2053\n",
      "          1       0.17      1.00      0.29       410\n",
      "\n",
      "avg / total       0.03      0.17      0.05      2463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda5.2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([x2_test, x2_test])\n",
    "x2_test.shape, prediction[1].shape, prediction[1].shape\n",
    "\n",
    "prediction = np.argmax(prediction[1], axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_racism_test, prediction))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_racism_test, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
