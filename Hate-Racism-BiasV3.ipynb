{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, load_model, Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import LeakyReLU, concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import Stemmer\n",
    "\n",
    "# Other\n",
    "import os,re\n",
    "import timeit\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import model_selection\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.engine import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "def reverse_gradient(X, hp_lambda):\n",
    "    '''Flips the sign of the incoming gradient during training.'''\n",
    "    try:\n",
    "        reverse_gradient.num_calls += 1\n",
    "    except AttributeError:\n",
    "        reverse_gradient.num_calls = 1\n",
    "\n",
    "    grad_name = \"GradientReversal%d\" % reverse_gradient.num_calls\n",
    "\n",
    "    @tf.RegisterGradient(grad_name)\n",
    "    def _flip_gradients(op, grad):\n",
    "        return [tf.negative(grad) * hp_lambda]\n",
    "\n",
    "    g = K.get_session().graph\n",
    "    with g.gradient_override_map({'Identity': grad_name}):\n",
    "        y = tf.identity(X)\n",
    "\n",
    "    return y\n",
    "\n",
    "class GradientReversal(Layer):\n",
    "    '''Flip the sign of gradient during training.'''\n",
    "    def __init__(self, hp_lambda, **kwargs):\n",
    "        super(GradientReversal, self).__init__(**kwargs)\n",
    "        self.supports_masking = False\n",
    "        self.hp_lambda = hp_lambda\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.trainable_weights = []\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return reverse_gradient(x, self.hp_lambda)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'hp_lambda': self.hp_lambda}\n",
    "        base_config = super(GradientReversal, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12315 entries, 0 to 12314\n",
      "Data columns (total 4 columns):\n",
      "id       12315 non-null int64\n",
      "label    12315 non-null object\n",
      "name     12315 non-null object\n",
      "text     12315 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 384.9+ KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>572342978255048705</td>\n",
       "      <td>racism</td>\n",
       "      <td>thefoxbandit</td>\n",
       "      <td>so drasko just said he was impressed the girls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>572341498827522049</td>\n",
       "      <td>racism</td>\n",
       "      <td>patricia</td>\n",
       "      <td>drasko they didn't cook half a bird you idiot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>572340476503724032</td>\n",
       "      <td>racism</td>\n",
       "      <td>food</td>\n",
       "      <td>hopefully someone cooks drasko in the next ep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>572334712804384768</td>\n",
       "      <td>racism</td>\n",
       "      <td>lil</td>\n",
       "      <td>of course you were born in serbia...you're as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>572332655397629952</td>\n",
       "      <td>racism</td>\n",
       "      <td>jlwhitaker</td>\n",
       "      <td>these girls are the equivalent of the irritati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>575949086055997440</td>\n",
       "      <td>racism</td>\n",
       "      <td>dominiquew</td>\n",
       "      <td>#mkr  lost the plot - where's the big texan wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>551659627872415744</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@lauracdean i love how the islamofascists recr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>551763146877452288</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@ibnhlophe @eeviewonders @anjemchoudary a crim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>551768543277355009</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary nothing desperate about it. hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>551769061055811584</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary idiots like you making such dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>551769698917167104</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary your prophet was a rapist, murd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>551779988567891969</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@anjemchoudary maybe you can give a talk about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>551783732466577408</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@jonsnowc4 @swatigauri when an islamolunatic b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>551787027725705216</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 israel is only trying to defend itsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>551787273256050688</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 the muslims have exterminated christi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>551787717168619521</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@remy119 muslims are fighting with every relig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>551796768623435776</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>it seems that allah sits around all day obsess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>551797843472556032</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>in islam women must be locked in their houses,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>551798472827883520</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>islam considers women as one big sex organ, so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>551799120080293888</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>to muslims a woman walking down the street is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id   label          name  \\\n",
       "0   572342978255048705  racism  thefoxbandit   \n",
       "1   572341498827522049  racism      patricia   \n",
       "2   572340476503724032  racism          food   \n",
       "3   572334712804384768  racism           lil   \n",
       "4   572332655397629952  racism    jlwhitaker   \n",
       "5   575949086055997440  racism    dominiquew   \n",
       "6   551659627872415744  racism    vile_islam   \n",
       "7   551763146877452288  racism    vile_islam   \n",
       "8   551768543277355009  racism    vile_islam   \n",
       "9   551769061055811584  racism    vile_islam   \n",
       "10  551769698917167104  racism    vile_islam   \n",
       "11  551779988567891969  racism    vile_islam   \n",
       "12  551783732466577408  racism    vile_islam   \n",
       "13  551787027725705216  racism    vile_islam   \n",
       "14  551787273256050688  racism    vile_islam   \n",
       "15  551787717168619521  racism    vile_islam   \n",
       "16  551796768623435776  racism    vile_islam   \n",
       "17  551797843472556032  racism    vile_islam   \n",
       "18  551798472827883520  racism    vile_islam   \n",
       "19  551799120080293888  racism    vile_islam   \n",
       "\n",
       "                                                 text  \n",
       "0   so drasko just said he was impressed the girls...  \n",
       "1   drasko they didn't cook half a bird you idiot ...  \n",
       "2   hopefully someone cooks drasko in the next ep ...  \n",
       "3   of course you were born in serbia...you're as ...  \n",
       "4   these girls are the equivalent of the irritati...  \n",
       "5   #mkr  lost the plot - where's the big texan wi...  \n",
       "6   @lauracdean i love how the islamofascists recr...  \n",
       "7   @ibnhlophe @eeviewonders @anjemchoudary a crim...  \n",
       "8   @anjemchoudary nothing desperate about it. hum...  \n",
       "9   @anjemchoudary idiots like you making such dec...  \n",
       "10  @anjemchoudary your prophet was a rapist, murd...  \n",
       "11  @anjemchoudary maybe you can give a talk about...  \n",
       "12  @jonsnowc4 @swatigauri when an islamolunatic b...  \n",
       "13  @remy119 israel is only trying to defend itsel...  \n",
       "14  @remy119 the muslims have exterminated christi...  \n",
       "15  @remy119 muslims are fighting with every relig...  \n",
       "16  it seems that allah sits around all day obsess...  \n",
       "17  in islam women must be locked in their houses,...  \n",
       "18  islam considers women as one big sex organ, so...  \n",
       "19  to muslims a woman walking down the street is ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_path = \"encoded_data/\"\n",
    "\n",
    "tweet_df = pd.read_pickle(encoded_data_path + \"tweets.pkl\")\n",
    "\n",
    "tweet_df.info()\n",
    "print(\"\\n\")\n",
    "tweet_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id\n",
      "count  1.231500e+04\n",
      "mean   5.619213e+17\n",
      "std    4.051089e+16\n",
      "min    3.193999e+17\n",
      "25%    5.641296e+17\n",
      "50%    5.710391e+17\n",
      "75%    5.756380e+17\n",
      "max    6.847790e+17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neither    5850\n",
       "sexism     4341\n",
       "racism     2074\n",
       "both         50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df.describe())\n",
    "\n",
    "tweet_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603528285247635456</td>\n",
       "      <td>neither</td>\n",
       "      <td>trash</td>\n",
       "      <td>@themikewebster cool i feel petty but also he ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>564049824274989058</td>\n",
       "      <td>sexism</td>\n",
       "      <td>male</td>\n",
       "      <td>@bmackillopthio but, trump gets called a megal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563905276496334849</td>\n",
       "      <td>sexism</td>\n",
       "      <td>male</td>\n",
       "      <td>.@blackopal80 then what really opened my eyes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>563797959729352704</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>leo was freaking out because of the wind, so i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>567765262713298944</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@keegannyc and how about if we investigate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>568231567698472961</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@frozenwes i hate numpads.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>572108345504501760</td>\n",
       "      <td>sexism</td>\n",
       "      <td>gynecologic</td>\n",
       "      <td>rt @federaltyranny: sexist femi nazi aborts ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>573770498837872640</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@mystyr_e we'll have more info out about this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>574773417213173760</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@witnesstoallah the poor women do it because t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>495020834495676417</td>\n",
       "      <td>sexism</td>\n",
       "      <td>@yesyouresexist</td>\n",
       "      <td>rt @nearthetown: @viragobooks @guardian   work...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>572343058529828864</td>\n",
       "      <td>neither</td>\n",
       "      <td>✌</td>\n",
       "      <td>that #begacheese ad is so old i'm wondering if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>392322287203921920</td>\n",
       "      <td>sexism</td>\n",
       "      <td>@yesyouresexist</td>\n",
       "      <td>rt @jcsliva1999 @pjmcentee she is blond what d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>569224391914967040</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>i'm looking for a few people to help with @gga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>603686773164609536</td>\n",
       "      <td>sexism</td>\n",
       "      <td>video</td>\n",
       "      <td>#blameonenotall, because if all men can be jud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>571006220451991552</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@peerworker @evvykube you are changing your mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>575548222204178433</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@pnibbler @alternet @maxblumenthal mohammed ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>556962076153757696</td>\n",
       "      <td>racism</td>\n",
       "      <td>vile_islam</td>\n",
       "      <td>@politics_pr @foxnews islamic no go zones are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>602181135435706369</td>\n",
       "      <td>neither</td>\n",
       "      <td>golfertx</td>\n",
       "      <td>hate to think that's exactly where the meet co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>571404378692837377</td>\n",
       "      <td>sexism</td>\n",
       "      <td>m</td>\n",
       "      <td>@frootsicle he interrupted the girl who starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>596165409029496833</td>\n",
       "      <td>neither</td>\n",
       "      <td>misandrist</td>\n",
       "      <td>@warrior_tank randi#1266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id    label             name  \\\n",
       "0   603528285247635456  neither            trash   \n",
       "1   564049824274989058   sexism             male   \n",
       "2   563905276496334849   sexism             male   \n",
       "3   563797959729352704  neither       misandrist   \n",
       "4   567765262713298944   racism       vile_islam   \n",
       "5   568231567698472961  neither       misandrist   \n",
       "6   572108345504501760   sexism      gynecologic   \n",
       "7   573770498837872640  neither       misandrist   \n",
       "8   574773417213173760   racism       vile_islam   \n",
       "9   495020834495676417   sexism  @yesyouresexist   \n",
       "10  572343058529828864  neither                ✌   \n",
       "11  392322287203921920   sexism  @yesyouresexist   \n",
       "12  569224391914967040  neither       misandrist   \n",
       "13  603686773164609536   sexism            video   \n",
       "14  571006220451991552  neither       misandrist   \n",
       "15  575548222204178433   racism       vile_islam   \n",
       "16  556962076153757696   racism       vile_islam   \n",
       "17  602181135435706369  neither         golfertx   \n",
       "18  571404378692837377   sexism                m   \n",
       "19  596165409029496833  neither       misandrist   \n",
       "\n",
       "                                                 text  \n",
       "0   @themikewebster cool i feel petty but also he ...  \n",
       "1   @bmackillopthio but, trump gets called a megal...  \n",
       "2   .@blackopal80 then what really opened my eyes....  \n",
       "3   leo was freaking out because of the wind, so i...  \n",
       "4   @keegannyc and how about if we investigate the...  \n",
       "5                          @frozenwes i hate numpads.  \n",
       "6   rt @federaltyranny: sexist femi nazi aborts ma...  \n",
       "7   @mystyr_e we'll have more info out about this ...  \n",
       "8   @witnesstoallah the poor women do it because t...  \n",
       "9   rt @nearthetown: @viragobooks @guardian   work...  \n",
       "10  that #begacheese ad is so old i'm wondering if...  \n",
       "11  rt @jcsliva1999 @pjmcentee she is blond what d...  \n",
       "12  i'm looking for a few people to help with @gga...  \n",
       "13  #blameonenotall, because if all men can be jud...  \n",
       "14  @peerworker @evvykube you are changing your mi...  \n",
       "15  @pnibbler @alternet @maxblumenthal mohammed ra...  \n",
       "16  @politics_pr @foxnews islamic no go zones are ...  \n",
       "17  hate to think that's exactly where the meet co...  \n",
       "18  @frootsicle he interrupted the girl who starte...  \n",
       "19                           @warrior_tank randi#1266  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffling \n",
    "tweet_df = tweet_df.sample(frac=1).reset_index(drop=True)\n",
    "tweet_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id\n",
      "count  1.231500e+04\n",
      "mean   5.619213e+17\n",
      "std    4.051089e+16\n",
      "min    3.193999e+17\n",
      "25%    5.641296e+17\n",
      "50%    5.710391e+17\n",
      "75%    5.756380e+17\n",
      "max    6.847790e+17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "neither    5850\n",
       "sexism     4341\n",
       "racism     2074\n",
       "both         50\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tweet_df.describe())\n",
    "tweet_df['label'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!!!\n"
     ]
    }
   ],
   "source": [
    "op_file = \"dataset/cleaned_tweet_data.csv\"\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "stemmer = Stemmer.Stemmer('english', 100000)    \n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #Remove puncuation\n",
    "    #text = text.translate(string.punctuation)\n",
    "    \n",
    "    #split based on everything except a-z0-9_'.\\-\n",
    "    tokens = re.findall(\"[a-z0-9_'.\\-]+\", text.lower())\n",
    "    #tokens = text.lower().split()\n",
    "    \n",
    "    tokens = [stemmer.stemWord(w) for w in tokens if not w in stopWords and len(w) > 2 and len(w)<20]\n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def build_data(df):\n",
    "      \n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    #cleaning text\n",
    "    data['tweet'] = df['text'].map(lambda x: clean_text(x))    \n",
    "    \n",
    "    data['label'] = df['label']\n",
    "    \n",
    "    data['racism'] = df['label'].map(lambda x: 1 if x == 'racism' else 0)\n",
    "    data['sexism'] = df['label'].map(lambda x: 1 if x == 'sexism' else 0)\n",
    "    data['neither'] = df['label'].map(lambda x: 1 if x == 'neither' else 0)\n",
    "    data['both'] = df['label'].map(lambda x: 1 if x == 'both' else 0)\n",
    "    \n",
    "    # set hate 1 if label is racism/sexism/both\n",
    "    data['hate'] = df['label'].map(lambda x: 0 if x == 'neither' else 1)    \n",
    "    \n",
    "    #label =  {1:hate, 0:non-hate}\n",
    "    labels_hate = data['hate'].map(lambda x : 1 if int(x) == 1 else 0)\n",
    "    \n",
    "    #label =  {1:hate, 0:non-hate}\n",
    "    labels_racist = data['racism'].map(lambda x : 1 if int(x) == 1 else 0)\n",
    "    \n",
    "    data.to_csv(op_file)\n",
    "    \n",
    "    return (data, labels_hate, labels_racist)\n",
    "\n",
    "data, labels_hate, labels_racist  = build_data(tweet_df)\n",
    "print(\"Done!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12315 entries, 0 to 12314\n",
      "Data columns (total 7 columns):\n",
      "tweet      12315 non-null object\n",
      "label      12315 non-null object\n",
      "racism     12315 non-null int64\n",
      "sexism     12315 non-null int64\n",
      "neither    12315 non-null int64\n",
      "both       12315 non-null int64\n",
      "hate       12315 non-null int64\n",
      "dtypes: int64(5), object(2)\n",
      "memory usage: 673.6+ KB\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>racism</th>\n",
       "      <th>sexism</th>\n",
       "      <th>neither</th>\n",
       "      <th>both</th>\n",
       "      <th>hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>themikewebst cool feel petti also rli take sid...</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmackillopthio trump get call megalomaniac. ge...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackopal80 realli open eyes. articl financi a...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leo freak wind buri luggag could sleep. http t...</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>keegannyc investig hate speech quran. http t.c...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>frozenw hate numpads.</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>federaltyranni sexist femi nazi abort male bab...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>mystyr_ we'll info soon.</td>\n",
       "      <td>neither</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>witnesstoallah poor women marri muslim man pre...</td>\n",
       "      <td>racism</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nearthetown viragobook guardian work virtual f...</td>\n",
       "      <td>sexism</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet    label  racism  sexism  \\\n",
       "0  themikewebst cool feel petti also rli take sid...  neither       0       0   \n",
       "1  bmackillopthio trump get call megalomaniac. ge...   sexism       0       1   \n",
       "2  blackopal80 realli open eyes. articl financi a...   sexism       0       1   \n",
       "3  leo freak wind buri luggag could sleep. http t...  neither       0       0   \n",
       "4  keegannyc investig hate speech quran. http t.c...   racism       1       0   \n",
       "5                              frozenw hate numpads.  neither       0       0   \n",
       "6  federaltyranni sexist femi nazi abort male bab...   sexism       0       1   \n",
       "7                           mystyr_ we'll info soon.  neither       0       0   \n",
       "8  witnesstoallah poor women marri muslim man pre...   racism       1       0   \n",
       "9  nearthetown viragobook guardian work virtual f...   sexism       0       1   \n",
       "\n",
       "   neither  both  hate  \n",
       "0        1     0     0  \n",
       "1        0     0     1  \n",
       "2        0     0     1  \n",
       "3        1     0     0  \n",
       "4        0     0     1  \n",
       "5        1     0     0  \n",
       "6        0     0     1  \n",
       "7        1     0     0  \n",
       "8        0     0     1  \n",
       "9        0     0     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info()\n",
    "print(\"\\n\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find correlation between columns\n",
    "\n",
    "def plot_correlation(data, size=8):\n",
    "    corr= data.corr()\n",
    "    fig, ax =plt.subplots(figsize=(size,size))\n",
    "    ax.matshow(corr)\n",
    "    plt.xticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.yticks(range(len(corr.columns)),corr.columns)\n",
    "    plt.show()\n",
    "\n",
    "plot_correlation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neither    5850\n",
      "sexism     4341\n",
      "racism     2074\n",
      "both         50\n",
      "Name: label, dtype: int64\n",
      "\n",
      "hate(1) and non-hate(0) label count:-\n",
      "\n",
      "1    6465\n",
      "0    5850\n",
      "Name: hate, dtype: int64\n",
      "\n",
      "racism(1) and non-racism(0) label count:-\n",
      "\n",
      "0    10241\n",
      "1     2074\n",
      "Name: racism, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "print(\"\\nhate(1) and non-hate(0) label count:-\\n\")\n",
    "print(data['hate'].value_counts())\n",
    "\n",
    "\n",
    "print(\"\\nracism(1) and non-racism(0) label count:-\\n\")\n",
    "print(data['racism'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len_of_tweet: 20 \n",
      "\n",
      "ive met racist people. ive met sexist people. ive never met someon treat everi man like he rapist.. blameonenotal stupid.\n",
      "\n",
      "length of top 200 tweets:-\n",
      " [20, 20, 20, 20, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15]\n",
      "\n",
      "number of words:  116283\n",
      "number of unique words:  21351\n"
     ]
    }
   ],
   "source": [
    "# max tweet length\n",
    "\n",
    "tweets = data['tweet'].values\n",
    "\n",
    "max_len = 0\n",
    "max_len_tweet = \"\"\n",
    "len_data = []\n",
    "words = []\n",
    "\n",
    "for t in tweets:\n",
    "    len_data.append(len(t.split()))\n",
    "    words += t.split()\n",
    "    if len(t.split()) > max_len:\n",
    "        max_len = len(t.split())\n",
    "        max_len_tweet = t\n",
    "        \n",
    "\n",
    "VOCAB_SIZE = len(set(words))\n",
    "        \n",
    "print(\"max_len_of_tweet:\", max_len,\"\\n\")\n",
    "print(max_len_tweet)\n",
    "\n",
    "print(\"\\nlength of top 200 tweets:-\\n\",sorted(len_data, reverse = True)[:400])\n",
    "\n",
    "print(\"\\nnumber of words: \", len(words))\n",
    "print(\"number of unique words: \", VOCAB_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sample shape: (12315, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12315, 20)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "MAX_SENT_LEN2 = 20\n",
    "\n",
    "def process_data2(sample):  \n",
    "    # Keras tokenizer function to tokenize the strings and \n",
    "    #‘texts_to_sequences’ to make sequences of words.\n",
    "\n",
    "    #Maximum number of words to work with \n",
    "    #if set, tokenization will be restricted to the top nb_words most common words in the dataset).\n",
    "    tokenizer = Tokenizer(num_words= VOCAB_SIZE)\n",
    "\n",
    "    #fit_on_texts(texts):\n",
    "    #Arguments: list of texts to train on.\n",
    "    #tokenizer.fit_on_texts(data['tweet'])\n",
    "    tokenizer.fit_on_texts(sample)\n",
    "\n",
    "    #texts_to_sequences(texts)\n",
    "    #texts: list of texts to turn to sequences.\n",
    "    #Return: list of sequences (one per text input).\n",
    "    \n",
    "    #sequences = tokenizer.texts_to_sequences(data['tweet'])\n",
    "    sequences = tokenizer.texts_to_sequences(sample)\n",
    "    sample = pad_sequences(sequences, maxlen = MAX_SENT_LEN2)\n",
    "    \n",
    "    print(\"Processed sample shape:\", sample.shape)\n",
    "    #print(\"Sample1:\", sample[0])\n",
    "    \n",
    "    return sample\n",
    "\n",
    "\n",
    "data_embedding2 = process_data2(data['tweet'])\n",
    "data_embedding2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "themikewebst cool feel petti also rli take side someon call feminazi time so....... \n",
      "\n",
      "[   0    0    0    0    0    0    0 7909  501   88 4160  116 3429   56\n",
      "  986   65   15    7   25  439]\n"
     ]
    }
   ],
   "source": [
    "print(data['tweet'][0], \"\\n\")\n",
    "print(data_embedding2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_size = int(len(data_embedding2)*.8)\n",
    "\n",
    "x2 = data_embedding2\n",
    "\n",
    "# converting to numpy array\n",
    "#y = labels.values\n",
    "y2_hate = labels_hate\n",
    "y2_racism = labels_racist\n",
    "\n",
    "#x2_train, x2_test, y2_train, y2_test = train_test_split(x2, y2, test_size = split_size)\n",
    "\n",
    "x2_train = x2[:split_size]\n",
    "x2_test = x2[split_size:]\n",
    "\n",
    "y2_hate_train = y2_hate[:split_size]\n",
    "y2_hate_test = y2_hate[split_size:]\n",
    "\n",
    "y2_racism_train = y2_racism[:split_size]\n",
    "y2_racism_test = y2_racism[split_size:]\n",
    "\n",
    "# transform labels into one hot representation\n",
    "y2_hate_train_one_hot = (np.arange(np.max(y2_hate_train) + 1) == y2_hate_train[:, None]).astype(float)\n",
    "y2_hate_test_one_hot = (np.arange(np.max(y2_hate_test) + 1) == y2_hate_test[:, None]).astype(float)\n",
    "\n",
    "y2_racism_train_one_hot = (np.arange(np.max(y2_racism_train) + 1) == y2_racism_train[:, None]).astype(float)\n",
    "y2_racism_test_one_hot = (np.arange(np.max(y2_racism_test) + 1) == y2_racism_test[:, None]).astype(float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate Train split:\n",
      "1    5172\n",
      "0    4680\n",
      "Name: hate, dtype: int64\n",
      "1    0.52497\n",
      "0    0.47503\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "\n",
      "Hate Test split:\n",
      "1    1293\n",
      "0    1170\n",
      "Name: hate, dtype: int64\n",
      "1    0.52497\n",
      "0    0.47503\n",
      "Name: hate, dtype: float64\n",
      "\n",
      "Racism Train split:\n",
      "0    8202\n",
      "1    1650\n",
      "Name: racism, dtype: int64\n",
      "0    0.832521\n",
      "1    0.167479\n",
      "Name: racism, dtype: float64\n",
      "\n",
      "\n",
      "Racism Test split:\n",
      "0    2039\n",
      "1     424\n",
      "Name: racism, dtype: int64\n",
      "0    0.827852\n",
      "1    0.172148\n",
      "Name: racism, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Hate Train split:\")\n",
    "print(y2_hate_train.value_counts())\n",
    "print(y2_hate_train.value_counts()/len(y2_hate_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Hate Test split:\")\n",
    "print(y2_hate_test.value_counts())\n",
    "print(y2_hate_test.value_counts()/len(y2_hate_test))\n",
    "\n",
    "\n",
    "print(\"\\nRacism Train split:\")\n",
    "print(y2_racism_train.value_counts())\n",
    "print(y2_racism_train.value_counts()/len(y2_racism_train))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Racism Test split:\")\n",
    "print(y2_racism_test.value_counts())\n",
    "print(y2_racism_test.value_counts()/len(y2_racism_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 9852 samples, validate on 2463 samples\n",
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 57s 6ms/step - loss: 0.3679 - acc: 0.8302 - val_loss: 0.2564 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.89931, saving model to encoded_data/weights-improvement-01-0.8993.hdf5\n",
      "Epoch 2/10\n",
      "2080/9852 [=====>........................] - ETA: 42s - loss: 0.1343 - acc: 0.9534"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-621433b814d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               validation_data = (x2_test, y2_hate_test_one_hot))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 21351\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM, input_length = INP_DIM))\n",
    "model.add(LSTM(200, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"weights-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "\n",
    "model.fit(x2_train, y2_hate_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_hate_test_one_hot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dense_11/Softmax:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 21351\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM, input_length = INP_DIM))\n",
    "model.add(LSTM(200, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463,) (2463, 2) <class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1024  142]\n",
      " [  91 1206]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.88      0.90      1166\n",
      "          1       0.89      0.93      0.91      1297\n",
      "\n",
      "avg / total       0.91      0.91      0.91      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm_hate = load_model(encoded_data_path +  'weights-improvement-02-0.9054.hdf5')\n",
    "\n",
    "y2_predicted = model_lstm_hate.predict(x2_test)\n",
    "print(y2_hate_test.shape, y2_predicted.shape, type(y2_hate_test), type(y2_predicted))\n",
    "\n",
    "y2_predicted_ = y2_predicted.argmax(axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_hate_test, y2_predicted_))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_hate_test, y2_predicted_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n",
      "Train on 9852 samples, validate on 2463 samples\n",
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 162s 16ms/step - loss: 0.1311 - acc: 0.9578 - val_loss: 0.0770 - val_acc: 0.9773\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.97726, saving model to encoded_data/weights-racism-improvement-01-0.9773.hdf5\n",
      "Epoch 2/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0206 - acc: 0.9938 - val_loss: 0.0904 - val_acc: 0.9781\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.97726 to 0.97808, saving model to encoded_data/weights-racism-improvement-02-0.9781.hdf5\n",
      "Epoch 3/10\n",
      "9852/9852 [==============================] - 149s 15ms/step - loss: 0.0112 - acc: 0.9975 - val_loss: 0.0988 - val_acc: 0.9765\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.97808\n",
      "Epoch 4/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.1174 - val_acc: 0.9712\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97808\n",
      "Epoch 5/10\n",
      "9852/9852 [==============================] - 148s 15ms/step - loss: 0.0036 - acc: 0.9987 - val_loss: 0.1319 - val_acc: 0.9793\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.97808 to 0.97929, saving model to encoded_data/weights-racism-improvement-05-0.9793.hdf5\n",
      "Epoch 6/10\n",
      "9852/9852 [==============================] - 152s 15ms/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.1524 - val_acc: 0.9777\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97929\n",
      "Epoch 7/10\n",
      " 152/9852 [..............................] - ETA: 2:29 - loss: 1.1686e-04 - acc: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1878189674cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               validation_data = (x2_test, y2_racism_test_one_hot))\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1399\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1400\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 21351\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_DIM, input_length = INP_DIM))\n",
    "model.add(LSTM(200, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"weights-racism-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "\n",
    "model.fit(x2_train, y2_racism_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_racism_test_one_hot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2463,) (2463, 2) <class 'pandas.core.series.Series'> <class 'numpy.ndarray'>\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2037   16]\n",
      " [  35  375]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99      2053\n",
      "          1       0.96      0.91      0.94       410\n",
      "\n",
      "avg / total       0.98      0.98      0.98      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lstm_racism = load_model(encoded_data_path +  'weights-racism-improvement-05-0.9793.hdf5')\n",
    "\n",
    "y2_predicted = model_lstm_racism.predict(x2_test)\n",
    "print(y2_racism_test.shape, y2_predicted.shape, type(y2_racism_test), type(y2_predicted))\n",
    "\n",
    "y2_predicted_ = y2_predicted.argmax(axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_racism_test, y2_predicted_))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_racism_test, y2_predicted_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_lstm_racism' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-9e71d09a5e96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_lstm_racism\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model_lstm_racism' is not defined"
     ]
    }
   ],
   "source": [
    "model_lstm_racism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "9852/9852 [==============================] - 142s 14ms/step - loss: -12.4621 - hate_output_loss: 0.5704 - race_output_loss: 13.0324\n",
      "Epoch 2/10\n",
      "9852/9852 [==============================] - 126s 13ms/step - loss: -13.1731 - hate_output_loss: 0.1709 - race_output_loss: 13.3440\n",
      "Epoch 3/10\n",
      "9852/9852 [==============================] - 139s 14ms/step - loss: -13.2903 - hate_output_loss: 0.0545 - race_output_loss: 13.3447\n",
      "Epoch 4/10\n",
      "9852/9852 [==============================] - 124s 13ms/step - loss: -13.3184 - hate_output_loss: 0.0271 - race_output_loss: 13.3455\n",
      "Epoch 5/10\n",
      "9852/9852 [==============================] - 127s 13ms/step - loss: -13.3251 - hate_output_loss: 0.0204 - race_output_loss: 13.3455\n",
      "Epoch 6/10\n",
      "9852/9852 [==============================] - 137s 14ms/step - loss: -13.3302 - hate_output_loss: 0.0153 - race_output_loss: 13.3455\n",
      "Epoch 7/10\n",
      "9852/9852 [==============================] - 132s 13ms/step - loss: -13.3323 - hate_output_loss: 0.0133 - race_output_loss: 13.3455\n",
      "Epoch 8/10\n",
      "9852/9852 [==============================] - 130s 13ms/step - loss: -13.3340 - hate_output_loss: 0.0115 - race_output_loss: 13.3455\n",
      "Epoch 9/10\n",
      "9852/9852 [==============================] - 123s 12ms/step - loss: -13.3317 - hate_output_loss: 0.0137 - race_output_loss: 13.3454\n",
      "Epoch 10/10\n",
      "9852/9852 [==============================] - 123s 12ms/step - loss: -13.3334 - hate_output_loss: 0.0122 - race_output_loss: 13.3455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xa2283b95c0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INP_DIM = x2_train.shape[1]\n",
    "\n",
    "input_hate = Input(shape=(INP_DIM,), name='input_hate')\n",
    "input_race = Input(shape=(INP_DIM,), name='input_race')\n",
    "\n",
    "x_hate = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_hate)\n",
    "x_race = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_race)\n",
    "\n",
    "lstm_hate_out = LSTM(100)(x_hate)\n",
    "lstm_race_out = LSTM(100)(x_race)\n",
    "\n",
    "x = concatenate([lstm_hate_out, lstm_race_out])\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "\n",
    "hate_output = Dense(2, activation='softmax', name='hate_output')(x)\n",
    "race_output = Dense(2, activation='softmax', name='race_output')(x)\n",
    "\n",
    "#This defines a model with two inputs and two outputs:\n",
    "model = Model(inputs=[input_hate, input_race], outputs=[hate_output, race_output])\n",
    "\n",
    "\n",
    "#We compile the model and assign a weight of 0.2 to the auxiliary loss. \n",
    "#To specify different loss_weights or lossfor each different output, you can use a list or a dictionary. \n",
    "#Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.\n",
    "\"\"\"\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "\n",
    "#We can train the model by passing it lists of input arrays and target arrays:\n",
    "model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "\"\"\"\n",
    "\n",
    "#Since our inputs and outputs are named (we passed them a \"name\" argument), \n",
    "#we could also have compiled the model via\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'hate_output': 'binary_crossentropy', 'race_output': 'binary_crossentropy'},\n",
    "              loss_weights={'hate_output': 1., 'race_output': -1.0})\n",
    "\n",
    "# And trained it via:\n",
    "model.fit({'input_hate': x2_train, 'input_race': x2_train},\n",
    "          {'hate_output': y2_hate_train_one_hot, 'race_output': y2_racism_train_one_hot},\n",
    "          epochs=10, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1083   83]\n",
      " [ 179 1118]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.93      0.89      1166\n",
      "          1       0.93      0.86      0.90      1297\n",
      "\n",
      "avg / total       0.90      0.89      0.89      2463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Confusion Matrix:\n",
    "[[1024  142]\n",
    " [  91 1206]]\n",
    "\n",
    "Classification Matrix:\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.92      0.88      0.90      1166\n",
    "          1       0.89      0.93      0.91      1297\n",
    "\n",
    "avg / total       0.91      0.91      0.91      2463\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prediction = model.predict([x2_test, x2_test])\n",
    "x2_test.shape, prediction[0].shape, prediction[1].shape\n",
    "\n",
    "prediction = np.argmax(prediction[0], axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_hate_test, prediction))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_hate_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[   0 2053]\n",
      " [   0  410]]\n",
      "\n",
      "Classification Matrix:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      2053\n",
      "          1       0.17      1.00      0.29       410\n",
      "\n",
      "avg / total       0.03      0.17      0.05      2463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda5.2\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([x2_test, x2_test])\n",
    "x2_test.shape, prediction[1].shape, prediction[1].shape\n",
    "\n",
    "prediction = np.argmax(prediction[1], axis=1)\n",
    "\n",
    "print ('\\nConfusion Matrix:')\n",
    "print (confusion_matrix(y2_racism_test, prediction))\n",
    "\n",
    "\n",
    "print ('\\nClassification Matrix:')\n",
    "print (classification_report(y2_racism_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_race_2:0\", shape=(?, 20), dtype=float32) at layer \"input_race\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0812bd983f13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#This defines a model with two inputs and two outputs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_hate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhate_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[0;32m     92\u001b[0m             \u001b[1;31m# Graph network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;31m# Subclassed network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[1;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[1;32m--> 231\u001b[1;33m             self.inputs, self.outputs)\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[1;34m(inputs, outputs)\u001b[0m\n\u001b[0;32m   1441\u001b[0m                                          \u001b[1;34m'The following previous layers '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1442\u001b[0m                                          \u001b[1;34m'were accessed without issue: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                          str(layers_with_complete_input))\n\u001b[0m\u001b[0;32m   1444\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mcomputable_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_race_2:0\", shape=(?, 20), dtype=float32) at layer \"input_race\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "INP_DIM = x2_train.shape[1]\n",
    "\n",
    "input_hate = Input(shape=(INP_DIM,), name='input_hate')\n",
    "input_race = Input(shape=(INP_DIM,), name='input_race')\n",
    "\n",
    "x_hate = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_hate)\n",
    "x_race = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_race)\n",
    "\n",
    "lstm_hate_out = LSTM(100)(x_hate)\n",
    "lstm_race_out = LSTM(100)(x_race)\n",
    "\n",
    "lstm_race_out = GradientReversal(.8)(lstm_race_out)\n",
    "\n",
    "x = concatenate([lstm_hate_out, lstm_race_out])\n",
    "\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x.add(Dense(2, activation='softmax', name='hate_output'))\n",
    "#x = Dense(64, activation='relu')(x)\n",
    "#hate_output = Dense(2, activation='softmax', name='hate_output')(x)\n",
    "\n",
    "#hate_output = Dense(2, activation='softmax', name='hate_output')(x)\n",
    "#race_output = Dense(2, activation='softmax', name='race_output')(x)\n",
    "\n",
    "#This defines a model with two inputs and two outputs:\n",
    "model = Model(inputs=input_hate, outputs=hate_output)\n",
    "\n",
    "\n",
    "#We compile the model and assign a weight of 0.2 to the auxiliary loss. \n",
    "#To specify different loss_weights or lossfor each different output, you can use a list or a dictionary. \n",
    "#Here we pass a single loss as the loss argument, so the same loss will be used on all outputs.\n",
    "\"\"\"\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n",
    "              loss_weights=[1., 0.2])\n",
    "\n",
    "#We can train the model by passing it lists of input arrays and target arrays:\n",
    "model.fit([headline_data, additional_data], [labels, labels],\n",
    "          epochs=50, batch_size=32)\n",
    "\"\"\"\n",
    "\n",
    "#Since our inputs and outputs are named (we passed them a \"name\" argument), \n",
    "#we could also have compiled the model via\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'hate_output': 'binary_crossentropy'},\n",
    "              loss_weights={'hate_output': 1})\n",
    "\n",
    "# And trained it via:\n",
    "model.fit({'input_hate': x2_train},\n",
    "          {'hate_output': y2_hate_train_one_hot},\n",
    "          epochs=10, batch_size=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training Model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[    0,     0,     0, ...,     7,    25,   439],\n       [    0,     0,     0, ...,     8,    15,  7911],\n       [    0,     0,     0, ...,  7913,   281,   548],\n       ...,\n       [    0,     ...",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-94a3962787fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m               \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallbacks_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m               validation_data = (x2_test, y2_racism_test_one_hot))\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 751\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    752\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda5.2\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;34m'Expected to see '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' array(s), '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;34m'but instead got the following list of '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[0;32m    103\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[    0,     0,     0, ...,     7,    25,   439],\n       [    0,     0,     0, ...,     8,    15,  7911],\n       [    0,     0,     0, ...,  7913,   281,   548],\n       ...,\n       [    0,     ..."
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE = 21351\n",
    "\n",
    "hp_lambda = .5\n",
    "INP_DIM = x2_train.shape[1]\n",
    "EMBED_DIM = 128\n",
    "OP_DIM = 2\n",
    "\n",
    "input_hate = Input(shape=(INP_DIM,), name='input_hate')\n",
    "x_hate = Embedding(output_dim=128, input_dim=VOCAB_SIZE, input_length = INP_DIM)(input_hate)\n",
    "lstm_hate_out = LSTM(100)(x_hate)\n",
    "\n",
    "Flip = GradientReversal(hp_lambda)\n",
    "\n",
    "dann_in = Flip(lstm_hate_out)\n",
    "dann_out = Dense(2, activation='softmax')(dann_in)\n",
    "\n",
    "\n",
    "#Flip = flipGradientTF.GradientReversal(hp_lambda)\n",
    "#dann_in = Flip(previous_layer_output)\n",
    "#dann_out = Dense(2)(dann_in)\n",
    "\n",
    "dann_out = Model(input_hate) \n",
    "dann_out.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "# creating checkpoint to save model every time validation accuracy improves\n",
    "filepath = encoded_data_path + \"weights-racism-improvement-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('\\n\\nTraining Model...')\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "#batch_size: Integer or None. Number of samples per gradient update. \n",
    "#If unspecified, batch_size will default to 32.\n",
    "\n",
    "model.fit(x2_train, y2_racism_train_one_hot,\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = EPOCHS,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_racism_test_one_hot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sfsfsf\n",
      "Train on 9852 samples, validate on 2463 samples\n",
      "Epoch 1/100\n",
      "9852/9852 [==============================] - 43s 4ms/step - loss: 0.3842 - acc: 0.8279 - val_loss: 0.2908 - val_acc: 0.8802 0.3874  - ETA: 1s -\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.52497 to 0.88023, saving model to encoded_data/weights-racism-improvement-01-0.8802.hdf5\n",
      "Epoch 2/100\n",
      "9852/9852 [==============================] - 43s 4ms/step - loss: 0.2587 - acc: 0.8997 - val_loss: 0.2779 - val_acc: 0.8863\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.88023 to 0.88632, saving model to encoded_data/weights-racism-improvement-02-0.8863.hdf5\n",
      "Epoch 3/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.2267 - acc: 0.9139 - val_loss: 0.2783 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.88632 to 0.89403, saving model to encoded_data/weights-racism-improvement-03-0.8940.hdf5\n",
      "Epoch 4/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.2036 - acc: 0.9235 - val_loss: 0.2660 - val_acc: 0.8936\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.89403\n",
      "Epoch 5/100\n",
      "9852/9852 [==============================] - 40s 4ms/step - loss: 0.1841 - acc: 0.9311 - val_loss: 0.2567 - val_acc: 0.894422 - ac -  - ETA: 1s - l\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.89403 to 0.89444, saving model to encoded_data/weights-racism-improvement-05-0.8944.hdf5\n",
      "Epoch 6/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.1643 - acc: 0.9415 - val_loss: 0.2605 - val_acc: 0.8977\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.89444 to 0.89769, saving model to encoded_data/weights-racism-improvement-06-0.8977.hdf5\n",
      "Epoch 7/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.1443 - acc: 0.9508 - val_loss: 0.3240 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.89769 to 0.89850, saving model to encoded_data/weights-racism-improvement-07-0.8985.hdf5\n",
      "Epoch 8/100\n",
      "9852/9852 [==============================] - 40s 4ms/step - loss: 0.1263 - acc: 0.9565 - val_loss: 0.2890 - val_acc: 0.9042\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.89850 to 0.90418, saving model to encoded_data/weights-racism-improvement-08-0.9042.hdf5\n",
      "Epoch 9/100\n",
      "9852/9852 [==============================] - 40s 4ms/step - loss: 0.1128 - acc: 0.9644 - val_loss: 0.3581 - val_acc: 0.8952\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.90418\n",
      "Epoch 10/100\n",
      "9852/9852 [==============================] - 40s 4ms/step - loss: 0.1007 - acc: 0.9673 - val_loss: 0.3408 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.90418\n",
      "Epoch 11/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.0867 - acc: 0.9728 - val_loss: 0.4239 - val_acc: 0.8993\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.90418\n",
      "Epoch 12/100\n",
      "9852/9852 [==============================] - 42s 4ms/step - loss: 0.0757 - acc: 0.9757 - val_loss: 0.4159 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.90418\n",
      "Epoch 13/100\n",
      "9852/9852 [==============================] - 47s 5ms/step - loss: 0.0718 - acc: 0.9773 - val_loss: 0.3672 - val_acc: 0.8831\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.90418\n",
      "Epoch 14/100\n",
      "9852/9852 [==============================] - 45s 5ms/step - loss: 0.0647 - acc: 0.9815 - val_loss: 0.4298 - val_acc: 0.8806\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.90418\n",
      "Epoch 15/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0591 - acc: 0.9839 - val_loss: 0.4776 - val_acc: 0.8871\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.90418\n",
      "Epoch 16/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0510 - acc: 0.9860 - val_loss: 0.4889 - val_acc: 0.8888\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.90418\n",
      "Epoch 17/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0455 - acc: 0.9858 - val_loss: 0.7230 - val_acc: 0.8303\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.90418\n",
      "Epoch 18/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0438 - acc: 0.9882 - val_loss: 0.6196 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.90418\n",
      "Epoch 19/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0378 - acc: 0.9898 - val_loss: 0.5783 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.90418\n",
      "Epoch 20/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0355 - acc: 0.9913 - val_loss: 0.6419 - val_acc: 0.8786\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.90418\n",
      "Epoch 21/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0372 - acc: 0.9904 - val_loss: 0.6964 - val_acc: 0.8689\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.90418\n",
      "Epoch 22/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0332 - acc: 0.9910 - val_loss: 0.6490 - val_acc: 0.8364\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.90418\n",
      "Epoch 23/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0338 - acc: 0.9919 - val_loss: 0.9091 - val_acc: 0.8725\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.90418\n",
      "Epoch 24/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0319 - acc: 0.9931 - val_loss: 0.7458 - val_acc: 0.8457\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.90418\n",
      "Epoch 25/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0284 - acc: 0.9923 - val_loss: 0.7928 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.90418\n",
      "Epoch 26/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0304 - acc: 0.9933 - val_loss: 0.8276 - val_acc: 0.8242\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.90418\n",
      "Epoch 27/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0261 - acc: 0.9934 - val_loss: 0.6875 - val_acc: 0.8445\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.90418\n",
      "Epoch 28/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0261 - acc: 0.9942 - val_loss: 1.0531 - val_acc: 0.8408\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.90418\n",
      "Epoch 29/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0261 - acc: 0.9940 - val_loss: 0.7714 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.90418\n",
      "Epoch 30/100\n",
      "9852/9852 [==============================] - 38s 4ms/step - loss: 0.0241 - acc: 0.9944 - val_loss: 1.0283 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.90418\n",
      "Epoch 31/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0254 - acc: 0.9948 - val_loss: 1.3973 - val_acc: 0.8283\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.90418\n",
      "Epoch 32/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0206 - acc: 0.9944 - val_loss: 0.9590 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.90418\n",
      "Epoch 33/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0237 - acc: 0.9946 - val_loss: 1.1938 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.90418\n",
      "Epoch 34/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0228 - acc: 0.9954 - val_loss: 1.3797 - val_acc: 0.8348\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.90418\n",
      "Epoch 35/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0250 - acc: 0.9949 - val_loss: 1.0360 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.90418\n",
      "Epoch 36/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0215 - acc: 0.9956 - val_loss: 1.4849 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.90418\n",
      "Epoch 37/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0215 - acc: 0.9956 - val_loss: 1.6806 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.90418\n",
      "Epoch 38/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0249 - acc: 0.9961 - val_loss: 1.4971 - val_acc: 0.8315\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.90418\n",
      "Epoch 39/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0228 - acc: 0.9956 - val_loss: 1.4306 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.90418\n",
      "Epoch 40/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0228 - acc: 0.9958 - val_loss: 1.7040 - val_acc: 0.8376\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.90418\n",
      "Epoch 41/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0266 - acc: 0.9958 - val_loss: 1.5861 - val_acc: 0.8303\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.90418\n",
      "Epoch 42/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0195 - acc: 0.9956 - val_loss: 1.7154 - val_acc: 0.8307\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.90418\n",
      "Epoch 43/100\n",
      "9852/9852 [==============================] - 38s 4ms/step - loss: 0.0210 - acc: 0.9961 - val_loss: 1.4880 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.90418\n",
      "Epoch 44/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0288 - acc: 0.9958 - val_loss: 1.9813 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.90418\n",
      "Epoch 45/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0212 - acc: 0.9955 - val_loss: 1.7804 - val_acc: 0.8380\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.90418\n",
      "Epoch 46/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0247 - acc: 0.9959 - val_loss: 1.8585 - val_acc: 0.8331\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.90418\n",
      "Epoch 47/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0212 - acc: 0.9962 - val_loss: 1.9781 - val_acc: 0.8339\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.90418\n",
      "Epoch 48/100\n",
      "9852/9852 [==============================] - 38s 4ms/step - loss: 0.0205 - acc: 0.9967 - val_loss: 1.7245 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.90418\n",
      "Epoch 49/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0234 - acc: 0.9970 - val_loss: 2.1373 - val_acc: 0.8262\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.90418\n",
      "Epoch 50/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0249 - acc: 0.9968 - val_loss: 2.1136 - val_acc: 0.8266\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.90418\n",
      "Epoch 51/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0208 - acc: 0.9964 - val_loss: 1.9828 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00051: val_acc did not improve from 0.90418\n",
      "Epoch 52/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0215 - acc: 0.9967 - val_loss: 2.0662 - val_acc: 0.8311\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.90418\n",
      "Epoch 53/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0235 - acc: 0.9963 - val_loss: 2.3304 - val_acc: 0.8165\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.90418\n",
      "Epoch 54/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0230 - acc: 0.9963 - val_loss: 2.3542 - val_acc: 0.8104\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.90418\n",
      "Epoch 55/100\n",
      "9852/9852 [==============================] - 39s 4ms/step - loss: 0.0250 - acc: 0.9969 - val_loss: 2.1664 - val_acc: 0.8238\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.90418\n",
      "Epoch 56/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.0175 - acc: 0.9971 - val_loss: 2.2896 - val_acc: 0.8015\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.90418\n",
      "Epoch 57/100\n",
      "9852/9852 [==============================] - 39s 4ms/step - loss: 0.0218 - acc: 0.9973 - val_loss: 2.4987 - val_acc: 0.7974\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.90418\n",
      "Epoch 58/100\n",
      "9852/9852 [==============================] - 37s 4ms/step - loss: 0.0253 - acc: 0.9968 - val_loss: 2.6995 - val_acc: 0.7970\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.90418\n",
      "Epoch 59/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0220 - acc: 0.9973 - val_loss: 2.4221 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.90418\n",
      "Epoch 60/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0203 - acc: 0.9971 - val_loss: 2.3902 - val_acc: 0.8035\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.90418\n",
      "Epoch 61/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0232 - acc: 0.9961 - val_loss: 2.3637 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.90418\n",
      "Epoch 62/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0267 - acc: 0.9963 - val_loss: 2.2763 - val_acc: 0.8047\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.90418\n",
      "Epoch 63/100\n",
      "9852/9852 [==============================] - 36s 4ms/step - loss: 0.0186 - acc: 0.9971 - val_loss: 2.6614 - val_acc: 0.7950\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.90418\n",
      "Epoch 64/100\n",
      "9852/9852 [==============================] - 35s 4ms/step - loss: 0.0231 - acc: 0.9969 - val_loss: 2.5734 - val_acc: 0.7933\n",
      "\n",
      "Epoch 00064: val_acc did not improve from 0.90418\n",
      "Epoch 65/100\n",
      "9852/9852 [==============================] - 42s 4ms/step - loss: 0.0231 - acc: 0.9963 - val_loss: 2.9479 - val_acc: 0.7795\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.90418\n",
      "Epoch 66/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.0248 - acc: 0.9967 - val_loss: 2.6961 - val_acc: 0.7864\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.90418\n",
      "Epoch 67/100\n",
      "9852/9852 [==============================] - 41s 4ms/step - loss: 0.0313 - acc: 0.9956 - val_loss: 2.3201 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.90418\n",
      "Epoch 68/100\n",
      "9852/9852 [==============================] - 38s 4ms/step - loss: 0.0318 - acc: 0.9957 - val_loss: 2.4284 - val_acc: 0.7929\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.90418\n",
      "Epoch 69/100\n",
      "9852/9852 [==============================] - 40s 4ms/step - loss: 0.0242 - acc: 0.9964 - val_loss: 1.9475 - val_acc: 0.8303\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.90418\n",
      "Epoch 70/100\n",
      "9852/9852 [==============================] - 45s 5ms/step - loss: 0.0229 - acc: 0.9962 - val_loss: 2.2018 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.90418\n",
      "Epoch 71/100\n",
      "9848/9852 [============================>.] - ETA: 0s - loss: 0.0220 - acc: 0.9970"
     ]
    }
   ],
   "source": [
    "# This returns a tensor\n",
    "inputs = Input(shape=(x2_train.shape[1],))\n",
    "\n",
    "x = Embedding(output_dim=25, input_dim=VOCAB_SIZE, input_length = INP_DIM)(inputs)\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = LSTM(32)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "#predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "Flip = GradientReversal(1)\n",
    "\n",
    "dann_in = Flip(x)\n",
    "dann_out = Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "print(\"sfsfsf\")\n",
    "\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "# predictions is made of an input layer and 3 dense layers\n",
    "model = Model(inputs=inputs, outputs=dann_out)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x2_train, y2_hate_train_one_hot,\n",
    "              batch_size = 8,\n",
    "              epochs = 100,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_hate_test_one_hot))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x000000A21E0DDF60>\n"
     ]
    }
   ],
   "source": [
    "# This returns a tensor\n",
    "inputs = Input(shape=(x2_train.shape[1],))\n",
    "\n",
    "x = Embedding(output_dim=25, input_dim=VOCAB_SIZE, input_length = INP_DIM)(inputs)\n",
    "\n",
    "# a layer instance is callable on a tensor, and returns a tensor\n",
    "x = LSTM(32)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "\n",
    "#predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "Flip = GradientReversal(1)\n",
    "\n",
    "dann_in = Flip(x)\n",
    "dann_out = Dense(2, activation='softmax')(x)\n",
    "\n",
    "\n",
    "print(\"sfsfsf\")\n",
    "\n",
    "\n",
    "# This creates a model that includes\n",
    "# the Input layer and three Dense layers\n",
    "# predictions is made of an input layer and 3 dense layers\n",
    "model = Model(inputs=inputs, outputs=dann_out)\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x2_train, y2_hate_train_one_hot,\n",
    "              batch_size = 8,\n",
    "              epochs = 100,\n",
    "              callbacks = callbacks_list,\n",
    "              validation_data = (x2_test, y2_hate_test_one_hot))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
